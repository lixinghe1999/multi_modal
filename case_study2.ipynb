{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This case study investigates whether we can perform two branch parallel, but merging two branch\n",
    "1. Linear\n",
    "2. Matmul\n",
    "\n",
    "Findings:\n",
    "1. Sparse not good\n",
    "2. CUDA Parallel only for small operation, don't have enough thread to do complete Parallel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from model.vit_model import Block\n",
    "def throughput(images, model):\n",
    "    model.eval()\n",
    "    batch_size, token_length = images[0].shape[0:2]\n",
    "    for i in range(50):\n",
    "        model(*images)\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "    for i in range(30):\n",
    "        model(*images)\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time()\n",
    "    print(f\"batch_size {batch_size} token_length {token_length} throughput {30 * batch_size / (tic2 - tic1)}\")\n",
    "    MB = 1024.0 * 1024.0\n",
    "    print('memory:', torch.cuda.max_memory_reserved() / MB)\n",
    "    return (tic2 - tic1) / 30"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, depth=12, embed_dim=768, stream=False, repeat=4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.stream = stream\n",
    "        self.repeat = repeat\n",
    "        self.model1 = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=12, mlp_ratio=4)\n",
    "            for _ in range(depth)])\n",
    "        self.model2 = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=12, mlp_ratio=4)\n",
    "            for _ in range(depth)])\n",
    "\n",
    "        self.s1 = torch.cuda.Stream(device=device)\n",
    "        self.s2 = torch.cuda.Stream(device=device)\n",
    "    def forward(self, x1, x2):\n",
    "        if self.stream:\n",
    "            with torch.cuda.stream(self.s1):\n",
    "                x1 = self.model1(x1)\n",
    "            with torch.cuda.stream(self.s2):\n",
    "                x2 = self.model2(x2)\n",
    "        else:\n",
    "            for blk1, blk2 in zip(self.model1, self.model2):\n",
    "                for i in range(self.repeat):\n",
    "                    x1 = blk1(x1)\n",
    "                    x2 = blk2(x2)\n",
    "\n",
    "        return x1, x2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class weight_fusion(DummyModel):\n",
    "    def __init__(self):\n",
    "        super(weight_fusion, self).__init__()\n",
    "        layers = []\n",
    "        for blk1, blk2 in zip(self.model1, self.model2):\n",
    "            weight = torch.zeros((2 * self.embed_dim, 2 * self.embed_dim))\n",
    "            #print(type(blk1.weight))\n",
    "            weight[:self.embed_dim, :self.embed_dim] = blk1.weight\n",
    "            weight[self.embed_dim:, self.embed_dim:] = blk2.weight\n",
    "            bias = torch.zeros(2 * self.embed_dim)\n",
    "            bias[:self.embed_dim] = blk1.bias\n",
    "            bias[self.embed_dim:] = blk2.bias\n",
    "            layer = nn.Linear(2 * self.embed_dim, 2 * self.embed_dim)\n",
    "            layer.weight = nn.Parameter(weight)\n",
    "            layer.bias = nn.Parameter(bias)\n",
    "            layers.append(layer)\n",
    "        self.model = nn.ModuleList(layers)\n",
    "    def forward(self, x):\n",
    "        for blk in self.model:\n",
    "            x = blk(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32 token_length 200 throughput 244.06631325974246\n",
      "memory: 11538.0\n",
      "0.13111190795898436\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "token_length = 200\n",
    "emb_dim = 300\n",
    "batch_size = 32\n",
    "non_blocking = True\n",
    "model = DummyModel(stream=False, embed_dim=emb_dim, repeat=1).to(device, non_blocking=non_blocking)\n",
    "data1 = torch.rand((batch_size, token_length, emb_dim)).to(device, non_blocking=non_blocking)\n",
    "data2 = torch.rand((batch_size, token_length, emb_dim)).to(device, non_blocking=non_blocking)\n",
    "latency = throughput((data1, data2), model)\n",
    "print(latency)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 4 token_length 10 throughput 1518.9369901708408\n",
      "memory: 2198.0\n",
      "batch_size 4 token_length 4 throughput 3333.3762492300966\n",
      "memory: 2198.0\n",
      "batch_size 4 token_length 0 throughput 9998.539502175252\n",
      "memory: 2198.0\n",
      "batch_size 4 token_length 0 throughput 9996.355114200596\n",
      "memory: 2198.0\n",
      "0.0002896005908648173\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "ratio = 0.4\n",
    "results = []\n",
    "token_length = 10\n",
    "for i in range(4):\n",
    "    token_length = int(token_length * (ratio ** i + 0.05))\n",
    "    model = DummyModel().to(device)\n",
    "    data1 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    data2 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    latency = throughput((data1, data2), model)\n",
    "    results.append(latency)\n",
    "print(sum(results)/4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32 token_length 200 throughput 643.430974057145\n",
      "memory: 1510.0\n",
      "batch_size 32 token_length 80 throughput 1526.2333964441866\n",
      "memory: 1510.0\n",
      "batch_size 32 token_length 12 throughput 9320.093049739948\n",
      "memory: 1510.0\n",
      "batch_size 32 token_length 0 throughput 191995.60556933054\n",
      "memory: 1510.0\n",
      "0.0005804698914289475\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "ratio = 0.4\n",
    "results = []\n",
    "token_length = 100\n",
    "model = weight_fusion().to(device)\n",
    "for i in range(4):\n",
    "    token_length = int(token_length * (ratio ** i))\n",
    "    data1 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    data2 = torch.rand((batch_size, token_length, emb_dim)).to(device)\n",
    "    data = torch.zeros((batch_size, 2 * token_length, 2 * emb_dim)).to(device)\n",
    "    data[:, :token_length, :emb_dim] = data1\n",
    "    data[:, token_length:, emb_dim:] = data2\n",
    "    latency = throughput([data], model)\n",
    "    results.append(latency)\n",
    "print(sum(results)/4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import sparselinear as sl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sl1 = sl.SparseLinear(20000, 20000, sparsity=.99).cuda()\n",
    "# Reduce weight dimensions if memory errors are raised\n",
    "fc1 = nn.Linear(20000, 20000).cuda()\n",
    "x = torch.rand(20000, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%timeit y = sl1(x)\n",
    "%timeit y = fc1(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0,   1,   2,  ..., 197, 198, 199],\n        [  0,   0,   0,  ..., 199, 199, 199]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "# num_connections = 4\n",
    "input_dim = 2 * emb_dim\n",
    "output_dim = 2 * emb_dim\n",
    "col = torch.arange(input_dim).repeat_interleave(emb_dim).view(1,-1).long()\n",
    "row = torch.cat([torch.arange(emb_dim).repeat(emb_dim).view(1,-1), torch.arange(emb_dim, 2*emb_dim).repeat(emb_dim).view(1,-1)], dim=1)\n",
    "# row = torch.randint(low=0, high=output_dim, size=(input_dim*num_connections,)).view(1,-1).long()\n",
    "connections = torch.cat((row, col), dim=0)\n",
    "connections"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 ms ± 541 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "89.8 µs ± 605 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_sparse import spspmm\n",
    "device = 'cuda'\n",
    "\n",
    "emb_dim = 300\n",
    "col = torch.arange(emb_dim * 2).repeat_interleave(emb_dim).view(1,-1).long()\n",
    "row = torch.cat([torch.arange(emb_dim).repeat(emb_dim).view(1,-1), torch.arange(emb_dim, 2*emb_dim).repeat(emb_dim).view(1,-1)], dim=1)\n",
    "\n",
    "indexA = torch.cat((col, row), dim=0).to(device)\n",
    "valueA = torch.rand(2 * emb_dim * emb_dim).to(device)\n",
    "\n",
    "indexB = torch.cat((col, row), dim=0).to(device)\n",
    "valueB = torch.rand(2 * emb_dim * emb_dim).to(device)\n",
    "\n",
    "\n",
    "\n",
    "matrixA = torch.zeros((2 * emb_dim, 2 * emb_dim)).to(device)\n",
    "dataA = valueA.reshape(emb_dim, -1)\n",
    "matrixA[:emb_dim, :emb_dim] = dataA[:, :emb_dim]\n",
    "matrixA[emb_dim:, emb_dim:] = dataA[:, emb_dim:]\n",
    "\n",
    "matrixB = torch.zeros((2 * emb_dim, 2 * emb_dim)).to(device)\n",
    "dataB = valueB.reshape(emb_dim, -1)\n",
    "matrixB[:emb_dim, :emb_dim] = dataB[:, :emb_dim]\n",
    "matrixB[emb_dim:, emb_dim:] = dataB[:, emb_dim:]\n",
    "\n",
    "%timeit spspmm(indexA, valueA, indexB, valueB, 2 * emb_dim, 2 * emb_dim, 2 * emb_dim)\n",
    "%timeit matrixA @ matrixB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "sl2 = sl.SparseLinear(input_dim, output_dim, connectivity=connections).cuda()\n",
    "fc2 = nn.Linear(input_dim, output_dim).cuda()\n",
    "\n",
    "t1, t2 = 100, 50\n",
    "data1 = torch.rand((4, t1, emb_dim)).to(device)\n",
    "data2 = torch.rand((4, t2, emb_dim)).to(device)\n",
    "data = torch.zeros((4, t1 + t2, input_dim)).to(device)\n",
    "data[:, :t1, :emb_dim] = data1\n",
    "data[:, t1:, emb_dim:] = data2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978 µs ± 21.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit y = sl2(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.48 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "71.7 µs ± 53.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit y = fc2(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 1.7449994087219238  seconds\n",
      "Inference took 1.1109998226165771  seconds\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "\n",
    "# In RNN parlance, the dimensions we care about are:\n",
    "# # of time-steps (T)\n",
    "# Batch size (B)\n",
    "# Hidden size/number of \"channels\" (C)\n",
    "T, B, C = 50, 50, 1024\n",
    "\n",
    "# A module that defines a single \"bidirectional LSTM\". This is simply two\n",
    "# LSTMs applied to the same sequence, but one in reverse\n",
    "class BidirectionalRecurrentLSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n",
    "        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        # Forward layer\n",
    "        output_f, _ = self.cell_f(x)\n",
    "\n",
    "        # Backward layer. Flip input in the time dimension (dim 0), apply the\n",
    "        # layer, then flip the outputs in the time dimension\n",
    "        x_rev = torch.flip(x, dims=[0])\n",
    "        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n",
    "        output_b_rev = torch.flip(output_b, dims=[0])\n",
    "\n",
    "        return torch.cat((output_f, output_b_rev), dim=2)\n",
    "\n",
    "# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n",
    "# ensemble are run one-by-one on the same input then their results are\n",
    "# stacked and summed together, returning the combined result.\n",
    "class LSTMEnsemble(torch.nn.Module):\n",
    "    def __init__(self, n_models):\n",
    "        super().__init__()\n",
    "        self.n_models = n_models\n",
    "        self.models = torch.nn.ModuleList([\n",
    "            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        results = []\n",
    "        for model in self.models:\n",
    "            results.append(model(x))\n",
    "        return torch.stack(results).sum(dim=0)\n",
    "class LSTMEnsemble_Parallel(torch.nn.Module):\n",
    "    def __init__(self, n_models):\n",
    "        super().__init__()\n",
    "        self.n_models = n_models\n",
    "        self.models = torch.nn.ModuleList([\n",
    "            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        futures = [torch.jit.fork(model, x) for model in self.models]\n",
    "        results = [torch.jit.wait(fut) for fut in futures]\n",
    "        return torch.stack(results).sum(dim=0)\n",
    "\n",
    "# For a head-to-head comparison to what we're going to do with fork/wait, let's\n",
    "# instantiate the model and compile it with TorchScript\n",
    "device = 'cpu'\n",
    "ens = torch.jit.script(LSTMEnsemble(n_models=4).to(device))\n",
    "ens_Parallel = torch.jit.script(LSTMEnsemble_Parallel(n_models=4).to(device))\n",
    "\n",
    "# Normally you would pull this input out of an embedding table, but for the\n",
    "# purpose of this demo let's just use random data.\n",
    "x = torch.rand(T, B, C).to(device)\n",
    "# Let's run the model once to warm up things like the memory allocator\n",
    "ens(x)\n",
    "\n",
    "x = torch.rand(T, B, C).to(device)\n",
    "# Let's see how fast it runs!\n",
    "s = time.time()\n",
    "ens(x)\n",
    "print('Inference took', time.time() - s, ' seconds')\n",
    "# Let's see how fast it runs!\n",
    "s = time.time()\n",
    "ens_Parallel(x)\n",
    "print('Inference took', time.time() - s, ' seconds')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}